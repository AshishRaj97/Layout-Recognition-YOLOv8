## ğŸ“ YOLOv8 Layout Recognition for Historical Documents  

This project is part of the RenAIssance Test I (Layout Organization Recognition). The goal is to develop a model that detects and segments the main text regions in scanned early modern printed sources, while ignoring embellishments like marginalia and decorations.
This project utilizes **YOLOv8** for **Layout Organization Recognition** in **historical scanned documents**.  
The model detects **text regions**, including **headings, main text, authors, and drop caps**, while ignoring embellishments. 

---

### ğŸš€ Features  

âœ” **Dataset:** Custom dataset of historical document images, manually annotated for text regions.  
âœ” **Model:** **YOLOv8m**, fine-tuned with custom **augmentation** & **hyperparameters**.  
âœ” **Training:** **180 epochs**, optimized learning rate, batch size, and augmentation techniques.  
âœ” **Evaluation:** Precision, Recall, **mAP@0.5**, **mAP@0.5:0.95**, and **training loss analysis**, IoU, and F1-score.  
âœ” **Inference:** Predicts layouts on **new document images** and **saves** structured outputs.  
âœ” **Visualization:** Training loss, **mAP trends, Precision-Recall curves, and Confusion Matrix**.  

---

### ğŸ“œ Dataset & Preprocessing

ğŸ”¹ Dataset: The dataset consists of 6 scanned early modern printed sources.
ğŸ”¹ Preprocessing: The PDFs were converted to images and annotated in Roboflow to label only the main text regions. The dataset was then exported in YOLOv8 format for training.
ğŸ”¹ Challenges: Variability in text layout, marginalia, and embellishments required precise annotation.

ğŸ“¥ **Download Dataset & YOLO Results:**  
- ğŸ“ [Dataset (Train, Test, Valid)](https://drive.google.com/drive/folders/1tvZZfsfFHPlLx26hQEDGcEnAJ6h9g0wm?usp=drive_link)  
- ğŸ“ [YOLO Results (Trained Weights & Predictions)](https://drive.google.com/drive/folders/1hjbZ72TodFKLEgVIRXIIpPbUUwb7eqom?usp=drive_link)

---

### ğŸ¯ Model Selection & Training Details

ğŸ”¹**Why YOLOv8?**
  - YOLOv8 offers high-speed object detection and performs well on small datasets.
  - The model is lightweight yet powerful for text region detection in historical documents.
ğŸ”¹ **Training Setup:**
  - Model: YOLOv8m
  - Optimizer: AdamW
  - Batch Size: 16
  - Epochs: 180
  - Learning Rate: 0.001
  - Augmentations: Rotation, Contrast Adjustments, Shearing, Perspective Transformations
  - Confidence Threshold & IoU: Tuned for better text region segmentation

ğŸ”¹ **Model Selection: Why YOLOv8 over Transformers?**
- The RenAIssance Project suggests using convolutional-recurrent, transformer-based, or self-supervised models. While transformer-based models like LayoutLM or Swin Transformer are effective for structured document parsing, they are computationally expensive and require larger datasets.
- Instead, YOLOv8m, a convolutional model, was chosen due to:
    - âœ” Efficient object detection, suitable for layout recognition in small datasets.
    - âœ” Fast inference speed, crucial for historical document processing.
    - âœ” Ability to detect localized text regions without needing OCR integration.
For future iterations, I aim to experiment with Vision Transformer (ViT)-based YOLO variants to improve performance further.

|Model	            |   Speed	 |   Accuracy  	|Computational Cost       |  Text Region Suitability       |
|-------------------|----------|--------------|-------------------------|--------------------------------|
|YOLOv8	            |âš¡ Fast	 |âœ… High	    |  ğŸ’° Low	                |ğŸ† Best for small datasets      |
|LayoutLM	          |ğŸ¢ Slow	 |âœ… High	    |ğŸ’°ğŸ’° Expensive	        |ğŸ‘ Good for structured layouts  |  
|Swin Transformer	  |ğŸ¢ Slow	 |ğŸ”¥ Very High	|ğŸ’°ğŸ’°ğŸ’° Very Expensive	  |âœ… Strong but needs large data  |

---

### ğŸ¯ Model Performance  

ğŸ“Œ **Final Evaluation Metrics:**  

| **Metric**    | **Value**  |
|--------------|-----------|
| Precision    | 85.4%     |
| Recall       | 86.3%     |
| mAP@0.5      | 91.2%     |
| mAP@0.5:0.95 | 75.9%     |
| F1-Score     | 85.8%     |

ğŸ”¹ **Why These Metrics?**
- Since layout recognition requires precise text region detection while ignoring embellishments, we use Precision, Recall, and F1-score to measure detection accuracy and balance between false positives and false negatives. mAP@0.5 and mAP@0.5:0.95 ensure robustness across different localization overlaps.

ğŸ“Œ **Evaluation Metrics:**
- **Precision (85.4%):** Measures the percentage of correctly detected text regions among all detected regions. A high precision ensures that the model minimizes false positives (incorrectly detecting embellishments or margins as text).
- **Recall (86.3%):** Represents how many actual text regions were correctly detected. A high recall means the model effectively captures most text areas, reducing false negatives (missed text regions).
- **mAP@0.5 (91.2%):** Measures detection accuracy with a 50% IoU threshold, evaluating how well the model identifies text regions. A high score means strong localization performance.
- **mAP@0.5:0.95 (75.9%):** A more stringent metric that averages mAP across multiple IoU thresholds (from 0.5 to 0.95), testing the modelâ€™s robustness. A score of 75.9% suggests that the model maintains reliable detection across varying levels of overlap.
- **F1-score (85.8%):** A balance between Precision & Recall, useful for layout recognition where both false positives (wrong elements detected as text) and false negatives (missed text regions) need to be minimized.
- **IoU (Intersection over Union):** Measures how well predicted text regions overlap with ground truth annotations. A higher IoU ensures more accurate text localization.

ğŸ“Š **Training Loss & mAP Trends Over Epochs:**  

![download](https://github.com/user-attachments/assets/82e2b6ef-e2c8-4684-9735-b0061bd7f2b2)

ğŸ“ˆ **Precision-Recall Curve:**  

![download](https://github.com/user-attachments/assets/e3e92478-d896-4f30-a716-0806564cf981)

âœ… Successful Layout Recognition:

![d2869d59-2563-4794-80c4-a8d21063e2c1](https://github.com/user-attachments/assets/7aa4ef01-18a2-46fc-96cb-8b542b2b54f2)

ğŸ“¥ To view all graphs and predicted images, check the **Predicted Images and Curves** folder in the repository.

---

### ğŸš€ Observations  
âœ” **Good model convergence** with minimal overfitting.  
âœ” **Improved recall & precision**, enhancing layout recognition accuracy.  
âœ” **Strong detection** of main text and drop caps, with minor misclassifications in headings and author labels.  
âœ” **mAP@0.5 = 91.2%** and **mAP@0.5:0.95 = 75.9%**, indicating reliable detection but scope for layout refinement.  

---

### ğŸ”„ Next Steps for Improvement  
âœ” **Expand dataset** (current: 52 images) and add more text variations.  
âœ” **Apply stronger augmentations** (rotation, shearing, perspective) to improve generalization.  
âœ” **Fine-tune confidence threshold & IoU ** for better text region segmentation.  
âœ” **Experiment with YOLOv8L or YOLOv8X** for improved feature extraction.  
âœ” **Implement OCR post-processing** to assess text extraction accuracy.  
âœ” **Optimize heading recognition & threshold settings** for enhanced classification. 

---

### âš™ï¸ Requirements  

- **Python 3.7+**
- **GPU (Recommended): A Tesla T4 (used in Colab) or any NVIDIA GPU with at least 8GB VRAM for faster training.**  
- **Ultralytics YOLOv8**  
- **PyTorch, Pandas, Matplotlib, Pillow (PIL)**

---

### ğŸš€ Installation  

#### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/AshishRaj97/Layout-Recognition-YOLOv8.git
```
#### 2ï¸âƒ£ Install Required Packages
```bash
pip install ultralytics
pip install torch
pip install pandas matplotlib pillow
```
---

### ğŸ“œ License  
This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.  

### ğŸ‘¨â€ğŸ’» Author  
ğŸ“Œ **Ashish Raj**  
ğŸ“§ [ar469492@gmail.com] | ğŸ–¥ï¸ [Your GitHub](https://github.com/your-username)  

ğŸ”¹ If you find this project useful, give it a â­ on GitHub! ğŸš€  
